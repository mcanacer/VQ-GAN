# Taming Transformers for High-Resolution Image Synthesis â€” JAX/Flax Implementation

This repository contains a from-scratch implementation of the paper:

> ** Taming Transformers for High-Resolution Image Synthesis **  
> (https://arxiv.org/abs/2012.09841)

## ğŸ Training First Stage (VQGAN)

```bash
python train_vqgan.py configs/celeba_vqgan.yaml
```

## ğŸ Training Second Stage (Transformer)

```bash
python train_transformer.py configs/celeba_transformer.yaml
```

## ğŸ¨ Inference

```bash
python inference.py configs/celeba_transformer.yaml
```

## ğŸ–¼ Sample Generated Images From CelebA

![Generated Image](gen_images/generated_image5.png)
![Generated Image](gen_images/generated_image10.png)
![Generated Image](gen_images/generated_image18.png)
![Generated Image](gen_images/generated_image19.png)
![Generated Image](gen_images/generated_image23.png)
![Generated Image](gen_images/generated_image39.png)
