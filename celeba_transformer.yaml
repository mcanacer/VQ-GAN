dataset_params:
  img_size: 128
  batch_size: 64
  num_workers: 4

model:
  base_learning_rate: 4.5e-6
  target: transformer.GPT
  params:
    seed: 61
    vocab_size: 1024
    block_size: 512
    n_layer: 24
    n_head: 16
    n_embed: 1024
    dropout: 0.0
  checkpoint_path: transformer.pkl
  epochs: 100

first_stage_config:
  target: model.VQGAN
  params:
    embedding_dim: 256
    num_embeddings: 1024
    output_channels: 3
    channel_multipliers: [1, 1, 2, 2, 4]
  checkpoint_path: vqgan_celeba.pkl

wandb:
  project: 'VQ-GAN-GPT'
  name: 'celebA'